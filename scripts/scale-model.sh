
#!/bin/bash
# Scale model deployment based on load

NAMESPACE="ai-models"
DEPLOYMENT="llama3-70b-vllm"

scale_up() {
    echo "ðŸ”¼ Scaling up $DEPLOYMENT..."
    kubectl scale deployment $DEPLOYMENT --replicas=2 -n $NAMESPACE
    kubectl wait --for=condition=available --timeout=600s deployment/$DEPLOYMENT -n $NAMESPACE
    echo "âœ… Scaled up successfully"
}

scale_down() {
    echo "ðŸ”½ Scaling down $DEPLOYMENT..."
    kubectl scale deployment $DEPLOYMENT --replicas=1 -n $NAMESPACE
    echo "âœ… Scaled down successfully"
}

get_metrics() {
    # Get current replicas
    REPLICAS=$(kubectl get deployment $DEPLOYMENT -n $NAMESPACE -o jsonpath='{.status.replicas}')
    
    # Get average CPU/GPU usage (simplified)
    echo "Current replicas: $REPLICAS"
    
    # Check if we need to scale based on pending requests or load
    # This is a simplified version - in production, use metrics from Prometheus
}

case "$1" in
    up)
        scale_up
        ;;
    down)
        scale_down
        ;;
    status)
        get_metrics
        ;;
    *)
        echo "Usage: $0 {up|down|status}"
        exit 1
        ;;
esac
